diff --git a/cityscapes_mtl_xtc.py b/cityscapes_mtl_xtc.py
index 753f8e6..fdba4f9 100644
--- a/cityscapes_mtl_xtc.py
+++ b/cityscapes_mtl_xtc.py
@@ -106,7 +106,7 @@ if opt.ssl_type == 'onelabel':
 cityscapes_train_set = Cityscapes_crop(root=dataset_path, train=True, augmentation=True, aug_twice=True, flip=True)
 cityscapes_test_set = Cityscapes(root=dataset_path, train=False)
 
-batch_size = 8
+batch_size = 16
 cityscapes_train_loader = torch.utils.data.DataLoader(
     dataset=cityscapes_train_set,
     batch_size=batch_size,
@@ -155,7 +155,7 @@ for epoch in range(start_epoch, total_epoch):
     cost_depth = AverageMeter()
     cityscapes_train_dataset = iter(cityscapes_train_loader)
     for k in range(train_batch):
-        train_data, train_label, train_depth, image_index, train_data1, train_label1, train_depth1, trans_params, flip = cityscapes_train_dataset.next()
+        train_data, train_label, train_depth, image_index, train_data1, train_label1, train_depth1, trans_params, flip = next(cityscapes_train_dataset)
         train_data, train_label = train_data.cuda(), train_label.type(torch.LongTensor).cuda()
         train_depth = train_depth.cuda()
         train_data1, train_label1 = train_data1.cuda(), train_label1.type(torch.LongTensor).cuda()
@@ -248,7 +248,7 @@ for epoch in range(start_epoch, total_epoch):
         with torch.no_grad():  # operations inside don't track history
             cityscapes_test_dataset = iter(cityscapes_test_loader)
             for k in range(test_batch):
-                test_data, test_label, test_depth = cityscapes_test_dataset.next()
+                test_data, test_label, test_depth = next(cityscapes_test_dataset)
                 test_data, test_label = test_data.cuda(),  test_label.type(torch.LongTensor).cuda()
                 test_depth = test_depth.cuda()
 
diff --git a/dataset/augmentation.py b/dataset/augmentation.py
new file mode 100644
index 0000000..2dfe210
--- /dev/null
+++ b/dataset/augmentation.py
@@ -0,0 +1,179 @@
+######
+# Augmentation on pytorch tensor
+######
+import matplotlib.pyplot as plt
+import numpy as np
+import torch
+from torch import Tensor
+import torchvision.transforms as T
+from torchvision.transforms import functional as F, InterpolationMode
+from typing import Dict, List, Optional, Tuple
+
+class ImgAugment(torch.nn.Module):
+    """
+    Reference: RandAugment
+    https://pytorch.org/vision/main/_modules/torchvision/transforms/autoaugment.html#RandAugment
+    """
+    def __init__(
+            self,
+            num_ops: int = 2,
+            magnitude: int = 6, # default 9, range from 0 ~ num_magnitude_bins
+            num_magnitude_bins: int = 31, # magnitude resolution
+            interpolation: InterpolationMode = InterpolationMode.NEAREST,
+            fill: Optional[List[float]] = None,
+            magnitude_sampling=True,
+        ) -> None:
+        super().__init__()
+        self.num_ops = num_ops
+        self.magnitude = magnitude
+        self.num_magnitude_bins = num_magnitude_bins
+        self.interpolation = interpolation
+        self.fill = fill
+        self.magnitude_sampling = magnitude_sampling
+        
+    def _augmentation_space(self, num_bins: int, image_size: Tuple[int, int]) -> Dict[str, Tuple[Tensor, bool]]:
+        return {
+            # op_name: (magnitudes, signed)
+            "Identity": (torch.tensor(0.0), False),
+            # "ShearX": (torch.linspace(0.0, 0.3, num_bins), True),
+            # "ShearY": (torch.linspace(0.0, 0.3, num_bins), True),
+            # "TranslateX": (torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins), True),
+            # "TranslateY": (torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins), True),
+            # "Rotate": (torch.linspace(0.0, 30.0, num_bins), True),
+            "Brightness": (torch.linspace(0.0, 0.9, num_bins), True),
+            "Color": (torch.linspace(0.0, 0.9, num_bins), True),
+            "Contrast": (torch.linspace(0.0, 0.9, num_bins), True),
+            "Sharpness": (torch.linspace(0.0, 0.9, num_bins), True),
+            "Posterize": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(), False),
+            "Solarize": (torch.linspace(255.0, 0.0, num_bins), False),
+            "AutoContrast": (torch.tensor(0.0), False),
+            "Equalize": (torch.tensor(0.0), False),
+        }
+    
+    def forward(self, img: Tensor) -> Tensor:
+        """
+            img (PIL Image or Tensor): Image to be transformed.
+
+        Returns:
+            PIL Image or Tensor: Transformed image.
+        """
+        fill = self.fill
+        # channels, height, width = F.get_dimensions(img)
+        channels, height, width = img.shape
+        if isinstance(img, Tensor):
+            if isinstance(fill, (int, float)):
+                fill = [float(fill)] * channels
+            elif fill is not None:
+                fill = [float(f) for f in fill]
+
+        op_meta = self._augmentation_space(self.num_magnitude_bins, (height, width))
+        for _ in range(self.num_ops):
+            op_index = int(torch.randint(len(op_meta), (1,)).item())
+            op_name = list(op_meta.keys())[op_index]
+            magnitudes, signed = op_meta[op_name]
+            if self.magnitude_sampling:
+                sampled_magnitude = torch.randint(self.magnitude, (1,)).item()
+            else:
+                sampled_magnitude = self.magnitude
+            magnitude = float(magnitudes[sampled_magnitude].item()) if magnitudes.ndim > 0 else 0.0
+            if signed and torch.randint(2, (1,)):
+                magnitude *= -1.0
+            img = _apply_op(img, op_name, magnitude, interpolation=self.interpolation, fill=fill)
+
+        return img
+
+
+
+def _apply_op(
+    img: Tensor, op_name: str, magnitude: float, interpolation: InterpolationMode, fill: Optional[List[float]]
+):
+    if op_name == "ShearX":
+        # magnitude should be arctan(magnitude)
+        # official autoaug: (1, level, 0, 0, 1, 0)
+        # https://github.com/tensorflow/models/blob/dd02069717128186b88afa8d857ce57d17957f03/research/autoaugment/augmentation_transforms.py#L290
+        # compared to
+        # torchvision:      (1, tan(level), 0, 0, 1, 0)
+        # https://github.com/pytorch/vision/blob/0c2373d0bba3499e95776e7936e207d8a1676e65/torchvision/transforms/functional.py#L976
+        img = F.affine(
+            img,
+            angle=0.0,
+            translate=[0, 0],
+            scale=1.0,
+            shear=[math.degrees(math.atan(magnitude)), 0.0],
+            interpolation=interpolation,
+            fill=fill,
+            center=[0, 0],
+        )
+    elif op_name == "ShearY":
+        # magnitude should be arctan(magnitude)
+        # See above
+        img = F.affine(
+            img,
+            angle=0.0,
+            translate=[0, 0],
+            scale=1.0,
+            shear=[0.0, math.degrees(math.atan(magnitude))],
+            interpolation=interpolation,
+            fill=fill,
+            center=[0, 0],
+        )
+    elif op_name == "TranslateX":
+        img = F.affine(
+            img,
+            angle=0.0,
+            translate=[int(magnitude), 0],
+            scale=1.0,
+            interpolation=interpolation,
+            shear=[0.0, 0.0],
+            fill=fill,
+        )
+    elif op_name == "TranslateY":
+        img = F.affine(
+            img,
+            angle=0.0,
+            translate=[0, int(magnitude)],
+            scale=1.0,
+            interpolation=interpolation,
+            shear=[0.0, 0.0],
+            fill=fill,
+        )
+    elif op_name == "Rotate":
+        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)
+    elif op_name == "Brightness":
+        img = F.adjust_brightness(img, 1.0 + magnitude)
+    elif op_name == "Color":
+        img = F.adjust_saturation(img, 1.0 + magnitude)
+    elif op_name == "Contrast":
+        img = F.adjust_contrast(img, 1.0 + magnitude)
+    elif op_name == "Sharpness":
+        img = F.adjust_sharpness(img, 1.0 + magnitude)
+    elif op_name == "Posterize":
+        if torch.is_tensor(img):
+            img = T.ToPILImage()(img)
+            img = F.posterize(img, int(magnitude))
+            img = T.ToTensor()(img)
+        else:
+            img = F.posterize(img, int(magnitude))
+    elif op_name == "Solarize":
+        if torch.is_tensor(img):
+            img = T.ToPILImage()(img)
+            img = F.solarize(img, magnitude)
+            img = T.ToTensor()(img)
+        else:
+            img = F.solarize(img, magnitude)
+    elif op_name == "AutoContrast":
+        img = F.autocontrast(img)
+    elif op_name == "Equalize":
+        if torch.is_tensor(img):
+            img = T.ToPILImage()(img)
+            img = F.equalize(img)
+            img = T.ToTensor()(img)
+        else:
+            img = F.equalize(img)
+    elif op_name == "Invert":
+        img = F.invert(img)
+    elif op_name == "Identity":
+        pass
+    else:
+        raise ValueError(f"The provided operator {op_name} is not recognized.")
+    return img
diff --git a/dataset/cityscapesssl.py b/dataset/cityscapesssl.py
index 7b34636..3338957 100644
--- a/dataset/cityscapesssl.py
+++ b/dataset/cityscapesssl.py
@@ -9,6 +9,7 @@ import torchvision.transforms as transforms
 from PIL import Image
 import random
 import torch.nn.functional as F
+from dataset.augmentation import ImgAugment
 
 
 
@@ -81,6 +82,7 @@ class Cityscapes_crop(Dataset):
             transforms.RandomHorizontalFlip(),
             transforms.ToTensor(),
         ])
+        self.augmenter = ImgAugment(num_ops = 2, magnitude = 6, num_magnitude_bins=31, magnitude_sampling=True)
 
         # R\read the data file
         if train:
@@ -140,6 +142,8 @@ class Cityscapes_crop(Dataset):
                 else:
                     image1, semantic1, depth1, flip = image, semantic, depth, 0
             image1, semantic1, depth1, trans_params = RandomScaleCrop()(image1, semantic1, depth1)
+            image = self.augmenter(image)
+            image1 = self.augmenter(image1)
             return image.type(torch.FloatTensor), semantic.type(torch.FloatTensor), depth.type(torch.FloatTensor), index, image1.type(torch.FloatTensor), semantic1.type(torch.FloatTensor), depth1.type(torch.FloatTensor), trans_params, flip
         if self.train:
             return image.type(torch.FloatTensor), semantic.type(torch.FloatTensor), depth.type(torch.FloatTensor), index
diff --git a/dataset/nyuv2ssl.py b/dataset/nyuv2ssl.py
index ecebd7f..916cc9f 100644
--- a/dataset/nyuv2ssl.py
+++ b/dataset/nyuv2ssl.py
@@ -9,7 +9,7 @@ import torchvision.transforms as transforms
 from PIL import Image
 import random
 import torch.nn.functional as F
-
+from dataset.augmentation import ImgAugment
 
 
 class RandomScaleCrop(object):
@@ -77,6 +77,7 @@ class NYUv2_crop(Dataset):
         self.root = os.path.expanduser(root)
         self.augmentation = augmentation
         self.aug_twice = aug_twice
+        self.augmenter = ImgAugment(num_ops = 2, magnitude = 6, num_magnitude_bins=31, magnitude_sampling=True)
 
 
         # R\read the data file
@@ -102,6 +103,8 @@ class NYUv2_crop(Dataset):
         elif self.augmentation and self.aug_twice:
             image, semantic, depth, normal, _ = RandomScaleCrop()(image, semantic, depth, normal)
             image1, semantic1, depth1, normal1, trans_params = RandomScaleCrop()(image, semantic, depth, normal)
+            image = self.augmenter(image)
+            image1 = self.augmenter(image1)
             return image.type(torch.FloatTensor), semantic.type(torch.FloatTensor), depth.type(torch.FloatTensor), normal.type(torch.FloatTensor), index, image1.type(torch.FloatTensor), semantic1.type(torch.FloatTensor), depth1.type(torch.FloatTensor), normal1.type(torch.FloatTensor), trans_params
         if self.train:
             return image.type(torch.FloatTensor), semantic.type(torch.FloatTensor), depth.type(torch.FloatTensor), normal.type(torch.FloatTensor), index
diff --git a/nyu_mtl_xtc.py b/nyu_mtl_xtc.py
index e81799a..f9f1748 100644
--- a/nyu_mtl_xtc.py
+++ b/nyu_mtl_xtc.py
@@ -110,7 +110,7 @@ elif opt.ssl_type == 'randomlabels':
 nyuv2_train_set = NYUv2_crop(root=dataset_path, train=True, augmentation=True, aug_twice=True)
 nyuv2_test_set = NYUv2(root=dataset_path, train=False)
 
-batch_size = 2
+batch_size = 4
 nyuv2_train_loader = torch.utils.data.DataLoader(
     dataset=nyuv2_train_set,
     batch_size=batch_size,
@@ -162,7 +162,7 @@ for epoch in range(start_epoch, total_epoch):
     cost_normal = AverageMeter()
     nyuv2_train_dataset = iter(nyuv2_train_loader)
     for k in range(train_batch):
-        train_data, train_label, train_depth, train_normal, image_index, train_data1, train_label1, train_depth1, train_normal1, trans_params = nyuv2_train_dataset.next()
+        train_data, train_label, train_depth, train_normal, image_index, train_data1, train_label1, train_depth1, train_normal1, trans_params = next(nyuv2_train_dataset)
         train_data, train_label = train_data.cuda(), train_label.type(torch.LongTensor).cuda()
         train_depth, train_normal = train_depth.cuda(), train_normal.cuda()
         train_data1, train_label1 = train_data1.cuda(), train_label1.type(torch.LongTensor).cuda()
@@ -264,7 +264,7 @@ for epoch in range(start_epoch, total_epoch):
         with torch.no_grad():  # operations inside don't track history
             nyuv2_test_dataset = iter(nyuv2_test_loader)
             for k in range(test_batch):
-                test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()
+                test_data, test_label, test_depth, test_normal = next(nyuv2_test_dataset)
                 test_data, test_label = test_data.cuda(),  test_label.type(torch.LongTensor).cuda()
                 test_depth, test_normal = test_depth.cuda(), test_normal.cuda()
 
